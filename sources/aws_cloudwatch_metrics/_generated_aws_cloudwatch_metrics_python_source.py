# ==============================================================================
# Merged Lakeflow Source: aws_cloudwatch_metrics
# ==============================================================================
# This file is auto-generated by tools/scripts/merge_python_source.py
# Do not edit manually. Make changes to the source files instead.
# ==============================================================================

from datetime import datetime, timedelta
from decimal import Decimal
from typing import (
    Any,
    Dict,
    Iterator,
    List,
)
import json
import time

from botocore.exceptions import ClientError
from pyspark.sql import Row
from pyspark.sql.datasource import DataSource, DataSourceReader, SimpleDataSourceStreamReader
from pyspark.sql.types import *
import base64
import boto3


def register_lakeflow_source(spark):
    """Register the Lakeflow Python source with Spark."""

    ########################################################
    # libs/utils.py
    ########################################################

    def _parse_struct(value: Any, field_type: StructType) -> Row:
        """Parse a dictionary into a PySpark Row based on StructType schema."""
        if not isinstance(value, dict):
            raise ValueError(f"Expected a dictionary for StructType, got {type(value)}")
        # Spark Python -> Arrow conversion require missing StructType fields to be assigned None.
        if value == {}:
            raise ValueError(
                "field in StructType cannot be an empty dict. "
                "Please assign None as the default value instead."
            )
        field_dict = {}
        for field in field_type.fields:
            if field.name in value:
                field_dict[field.name] = parse_value(value.get(field.name), field.dataType)
            elif field.nullable:
                field_dict[field.name] = None
            else:
                raise ValueError(f"Field {field.name} is not nullable but not found in the input")
        return Row(**field_dict)


    def _parse_array(value: Any, field_type: ArrayType) -> list:
        """Parse a list into a PySpark array based on ArrayType schema."""
        if not isinstance(value, list):
            if field_type.containsNull:
                return [parse_value(value, field_type.elementType)]
            raise ValueError(f"Expected a list for ArrayType, got {type(value)}")
        return [parse_value(v, field_type.elementType) for v in value]


    def _parse_map(value: Any, field_type: MapType) -> dict:
        """Parse a dictionary into a PySpark map based on MapType schema."""
        if not isinstance(value, dict):
            raise ValueError(f"Expected a dictionary for MapType, got {type(value)}")
        return {
            parse_value(k, field_type.keyType): parse_value(v, field_type.valueType)
            for k, v in value.items()
        }


    def _parse_string(value: Any) -> str:
        """Convert value to string."""
        return str(value)


    def _parse_integer(value: Any) -> int:
        """Convert value to integer."""
        if isinstance(value, str) and value.strip():
            return int(float(value)) if "." in value else int(value)
        if isinstance(value, (int, float)):
            return int(value)
        raise ValueError(f"Cannot convert {value} to integer")


    def _parse_float(value: Any) -> float:
        """Convert value to float."""
        return float(value)


    def _parse_decimal(value: Any) -> Decimal:
        """Convert value to Decimal."""
        return Decimal(value) if isinstance(value, str) and value.strip() else Decimal(str(value))


    def _parse_boolean(value: Any) -> bool:
        """Convert value to boolean."""
        if isinstance(value, str):
            lowered = value.lower()
            if lowered in ("true", "t", "yes", "y", "1"):
                return True
            if lowered in ("false", "f", "no", "n", "0"):
                return False
        return bool(value)


    def _parse_date(value: Any) -> datetime.date:
        """Convert value to date."""
        if isinstance(value, str):
            for fmt in ("%Y-%m-%d", "%m/%d/%Y", "%d-%m-%Y", "%Y/%m/%d"):
                try:
                    return datetime.strptime(value, fmt).date()
                except ValueError:
                    continue
            return datetime.fromisoformat(value).date()
        if isinstance(value, datetime):
            return value.date()
        raise ValueError(f"Cannot convert {value} to date")


    def _parse_timestamp(value: Any) -> datetime:
        """Convert value to timestamp."""
        if isinstance(value, str):
            ts_value = value.replace("Z", "+00:00") if value.endswith("Z") else value
            try:
                return datetime.fromisoformat(ts_value)
            except ValueError:
                for fmt in ("%Y-%m-%d %H:%M:%S", "%Y/%m/%d %H:%M:%S"):
                    try:
                        return datetime.strptime(ts_value, fmt)
                    except ValueError:
                        continue
        elif isinstance(value, (int, float)):
            return datetime.fromtimestamp(value)
        elif isinstance(value, datetime):
            return value
        raise ValueError(f"Cannot convert {value} to timestamp")


    def _decode_string_to_bytes(value: str) -> bytes:
        """Try to decode a string as base64, then hex, then UTF-8."""
        try:
            return base64.b64decode(value)
        except Exception:
            pass
        try:
            return bytes.fromhex(value)
        except Exception:
            pass
        return value.encode("utf-8")


    def _parse_binary(value: Any) -> bytes:
        """Convert value to bytes. Tries base64, then hex, then UTF-8 for strings."""
        if isinstance(value, bytes):
            return value
        if isinstance(value, bytearray):
            return bytes(value)
        if isinstance(value, str):
            return _decode_string_to_bytes(value)
        if isinstance(value, list):
            return bytes(value)
        return str(value).encode("utf-8")


    # Mapping of primitive types to their parser functions
    _PRIMITIVE_PARSERS = {
        StringType: _parse_string,
        IntegerType: _parse_integer,
        LongType: _parse_integer,
        FloatType: _parse_float,
        DoubleType: _parse_float,
        DecimalType: _parse_decimal,
        BooleanType: _parse_boolean,
        DateType: _parse_date,
        TimestampType: _parse_timestamp,
        BinaryType: _parse_binary,
    }


    def parse_value(value: Any, field_type: DataType) -> Any:
        """
        Converts a JSON value into a PySpark-compatible data type based on the provided field type.
        """
        if value is None:
            return None

        # Handle complex types
        if isinstance(field_type, StructType):
            return _parse_struct(value, field_type)
        if isinstance(field_type, ArrayType):
            return _parse_array(value, field_type)
        if isinstance(field_type, MapType):
            return _parse_map(value, field_type)

        # Handle primitive types via type-based lookup
        try:
            field_type_class = type(field_type)
            if field_type_class in _PRIMITIVE_PARSERS:
                return _PRIMITIVE_PARSERS[field_type_class](value)

            # Check for custom UDT handling
            if hasattr(field_type, "fromJson"):
                return field_type.fromJson(value)

            raise TypeError(f"Unsupported field type: {field_type}")
        except (ValueError, TypeError) as e:
            raise ValueError(f"Error converting '{value}' ({type(value)}) to {field_type}: {str(e)}")


    ########################################################
    # sources/aws_cloudwatch_metrics/aws_cloudwatch_metrics.py
    ########################################################

    class LakeflowConnect:
        def __init__(self, options: dict[str, str]) -> None:
            """
            Initialize the AWS CloudWatch Metrics connector with connection-level options.

            Expected options:
                - aws_access_key_id: AWS Access Key ID for authentication
                - aws_secret_access_key: AWS Secret Access Key for authentication
                - region: AWS region where CloudWatch metrics are stored
                - cloudwatch_metrics_namespace: CloudWatch namespace for metrics
            """
            aws_access_key_id = options.get("aws_access_key_id")
            aws_secret_access_key = options.get("aws_secret_access_key")
            region = options.get("region")
            self.namespace = options.get("cloudwatch_metrics_namespace")

            if not aws_access_key_id:
                raise ValueError("AWS CloudWatch connector requires 'aws_access_key_id' in options")
            if not aws_secret_access_key:
                raise ValueError("AWS CloudWatch connector requires 'aws_secret_access_key' in options")
            if not region:
                raise ValueError("AWS CloudWatch connector requires 'region' in options")
            if not self.namespace:
                raise ValueError("AWS CloudWatch connector requires 'cloudwatch_metrics_namespace' in options")

            # Initialize boto3 CloudWatch client
            self.client = boto3.client(
                'cloudwatch',
                aws_access_key_id=aws_access_key_id,
                aws_secret_access_key=aws_secret_access_key,
                region_name=region
            )
            self.region = region

            # Cache for discovered metrics (namespace + metric_name + dimensions combinations)
            self._discovered_metrics: List[Dict[str, Any]] | None = None

        def list_tables(self) -> list[str]:
            """
            List names of all tables supported by this connector.

            CloudWatch connector supports a single table: 'metrics'
            """
            return ["metrics"]

        def _discover_metrics(self, table_options: dict[str, str]) -> List[Dict[str, Any]]:
            """
            Discover all metrics and dimension combinations using ListMetrics API.

            Args:
                table_options: Table options that may include dimension filters

            Returns:
                List of metric dictionaries, each containing:
                - Namespace: CloudWatch namespace
                - MetricName: Name of the metric
                - Dimensions: List of dimension dictionaries with Name and Value keys
            """
            # If already discovered and no dimension filter, return cached
            if self._discovered_metrics is not None and "dimensions" not in table_options:
                return self._discovered_metrics

            # Check if dimensions filter is specified
            filter_dimensions = None
            if "dimensions" in table_options:
                filter_dimensions = table_options["dimensions"]
                if isinstance(filter_dimensions, str):
                    # Parse JSON string if provided as string
                    try:
                        filter_dimensions = json.loads(filter_dimensions)
                    except json.JSONDecodeError:
                        raise ValueError(f"Invalid dimensions format: {filter_dimensions}")

            # Build ListMetrics request
            list_params = {"Namespace": self.namespace}

            # If dimensions filter is specified, convert to DimensionFilters format
            if filter_dimensions:
                dimension_filters = []
                for key, value in filter_dimensions.items():
                    dimension_filters.append({
                        "Name": key,
                        "Value": value
                    })
                list_params["Dimensions"] = dimension_filters

            all_metrics = []
            next_token = None

            # Paginate through ListMetrics results
            while True:
                if next_token:
                    list_params["NextToken"] = next_token

                try:
                    response = self.client.list_metrics(**list_params)
                except ClientError as e:
                    error_code = e.response.get("Error", {}).get("Code", "")
                    if error_code == "Throttling":
                        # Retry with exponential backoff (simplified - in production use retry decorator)
                        time.sleep(1)
                        continue
                    raise RuntimeError(f"Failed to list metrics: {e}")

                metrics = response.get("Metrics", [])
                all_metrics.extend(metrics)

                next_token = response.get("NextToken")
                if not next_token:
                    break

            # Cache if no dimension filter
            if "dimensions" not in table_options:
                self._discovered_metrics = all_metrics

            return all_metrics

        def get_table_schema(
            self, table_name: str, table_options: dict[str, str]
        ) -> StructType:
            """
            Fetch the schema of the metrics table.

            Schema includes:
                - namespace: CloudWatch namespace
                - metric_name: Name of the metric
                - timestamp: Timestamp of the metric data point (UTC)
                - value: Metric value (double)
                - statistic: Statistic type (Average, Sum, Maximum, Minimum, SampleCount)
                - unit: Unit of the metric
                - dimensions: Dimensions as a map
                - region: AWS region (connector-derived)
            """
            if table_name != "metrics":
                raise ValueError(f"Unsupported table: {table_name!r}")

            return StructType(
                [
                    StructField("namespace", StringType(), False),
                    StructField("metric_name", StringType(), False),
                    StructField("timestamp", TimestampType(), False),
                    StructField("value", DoubleType(), True),
                    StructField("statistic", StringType(), False),
                    StructField("unit", StringType(), True),
                    StructField("dimensions", MapType(StringType(), StringType(), True), True),
                    StructField("region", StringType(), False),
                ]
            )

        def read_table_metadata(
            self, table_name: str, table_options: dict[str, str]
        ) -> dict:
            """
            Fetch metadata for the metrics table.

            Returns:
                - primary_keys: ["namespace", "metric_name", "timestamp", "dimensions"]
                - cursor_field: "timestamp"
                - ingestion_type: "append"
            """
            if table_name != "metrics":
                raise ValueError(f"Unsupported table: {table_name!r}")

            return {
                "primary_keys": ["namespace", "metric_name", "timestamp", "dimensions"],
                "cursor_field": "timestamp",
                "ingestion_type": "append",
            }

        def _parse_table_options(self, table_options: dict[str, str]) -> Dict[str, Any]:
            """
            Parse and validate table options.

            Args:
                table_options: Table options dictionary

            Returns:
                Dictionary with parsed options:
                - period: Period in seconds (multiple of 60)
                - statistics: List of statistics
                - unit: Unit string or None
            """
            # Parse period
            try:
                period = int(table_options.get("period", 300))
            except (TypeError, ValueError):
                period = 300
            # Ensure period is a multiple of 60
            if period % 60 != 0:
                period = (period // 60) * 60
                if period == 0:
                    period = 60

            # Parse statistics
            statistics = table_options.get("statistics", ["Average"])
            if isinstance(statistics, str):
                # Parse JSON string if provided as string
                try:
                    statistics = json.loads(statistics)
                except json.JSONDecodeError:
                    statistics = [statistics]

            if not isinstance(statistics, list):
                statistics = ["Average"]

            unit = table_options.get("unit")

            return {
                "period": period,
                "statistics": statistics,
                "unit": unit,
            }

        def _determine_time_range(self, start_offset: dict) -> tuple[datetime, datetime]:
            """
            Determine the time range for metric data retrieval.

            Args:
                start_offset: Start offset dictionary that may contain a cursor

            Returns:
                Tuple of (start_time, end_time) as UTC naive datetime objects
            """
            end_time = datetime.utcnow()

            if start_offset and isinstance(start_offset, dict):
                cursor = start_offset.get("cursor")
                if cursor:
                    try:
                        # Parse cursor as ISO 8601 timestamp
                        # Handle both 'Z' suffix and timezone-aware formats
                        cursor_clean = cursor.replace('Z', '+00:00')
                        if '+' not in cursor_clean and cursor_clean.count('-') > 2:
                            # Assume UTC if no timezone info
                            cursor_clean = cursor_clean + '+00:00'
                        start_time = datetime.fromisoformat(cursor_clean)
                        # Convert to UTC naive datetime if timezone-aware
                        if start_time.tzinfo is not None:
                            start_time = start_time.replace(tzinfo=None)
                        # Apply 5-minute lookback window
                        start_time = start_time - timedelta(minutes=5)
                    except (ValueError, AttributeError):
                        # If parsing fails, default to 60 minutes ago
                        start_time = end_time - timedelta(minutes=60)
                else:
                    # No cursor, first run - default to 60 minutes ago
                    start_time = end_time - timedelta(minutes=60)
            else:
                # No start_offset, first run - default to 60 minutes ago
                start_time = end_time - timedelta(minutes=60)

            return start_time, end_time

        def _build_metric_queries(
            self,
            discovered_metrics: List[Dict[str, Any]],
            period: int,
            statistics: List[str],
            unit: str | None,
        ) -> tuple[List[Dict[str, Any]], Dict[str, Dict[str, Any]]]:
            """
            Build MetricDataQueries and query metadata from discovered metrics.

            Args:
                discovered_metrics: List of metrics from ListMetrics API
                period: Period in seconds for metric aggregation
                statistics: List of statistics to retrieve
                unit: Unit for the metric (optional)

            Returns:
                Tuple of (metric_queries, query_metadata):
                - metric_queries: List of MetricDataQuery dictionaries
                - query_metadata: Dictionary mapping query ID to metric metadata
            """
            metric_queries = []
            query_metadata = {}

            query_id_counter = 1
            for metric in discovered_metrics:
                metric_name = metric.get("MetricName")
                dimensions = metric.get("Dimensions", [])

                # Convert dimensions to the format needed for GetMetricData
                dims_for_query = []
                dims_dict = {}
                for dim in dimensions:
                    dim_name = dim.get("Name")
                    dim_value = dim.get("Value")
                    if dim_name and dim_value:
                        dims_for_query.append({"Name": dim_name, "Value": dim_value})
                        dims_dict[dim_name] = dim_value

                # Create a query for each statistic
                for stat in statistics:
                    query_id = f"m{query_id_counter}"
                    query_id_counter += 1

                    metric_queries.append({
                        "Id": query_id,
                        "MetricStat": {
                            "Metric": {
                                "Namespace": self.namespace,
                                "MetricName": metric_name,
                                "Dimensions": dims_for_query
                            },
                            "Period": period,
                            "Stat": stat
                        },
                        "ReturnData": True
                    })

                    # Store metadata for this query
                    query_metadata[query_id] = {
                        "namespace": self.namespace,
                        "metric_name": metric_name,
                        "dimensions": dims_dict,
                        "statistic": stat,
                        "unit": unit  # May be None, will use default from API if not specified
                    }

            return metric_queries, query_metadata

        def _process_metric_data_results(
            self,
            metric_data_results: List[Dict[str, Any]],
            batch_metadata: Dict[str, Dict[str, Any]],
        ) -> tuple[List[dict], str | None]:
            """
            Process GetMetricData response results into records.

            Args:
                metric_data_results: List of MetricDataResult dictionaries from GetMetricData API
                batch_metadata: Dictionary mapping query ID to metric metadata

            Returns:
                Tuple of (records, max_timestamp):
                - records: List of record dictionaries
                - max_timestamp: Maximum timestamp string (ISO 8601) or None
            """
            records = []
            max_timestamp = None

            for result in metric_data_results:
                query_id = result.get("Id")
                metadata = batch_metadata.get(query_id)
                if not metadata:
                    continue

                timestamps = result.get("Timestamps", [])
                values = result.get("Values", [])

                # Skip if no data points
                if not timestamps or not values:
                    continue

                # Get unit from result if available, otherwise use metadata
                result_unit = result.get("Unit")
                if not result_unit:
                    result_unit = metadata.get("unit")

                # Create a record for each timestamp-value pair
                for ts, val in zip(timestamps, values):
                    # Convert timestamp to ISO 8601 string if it's a datetime object
                    if isinstance(ts, datetime):
                        ts_iso = ts.isoformat()
                    else:
                        ts_iso = str(ts)

                    record = {
                        "namespace": metadata["namespace"],
                        "metric_name": metadata["metric_name"],
                        "timestamp": ts_iso,
                        "value": float(val) if val is not None else None,
                        "statistic": metadata["statistic"],
                        "unit": result_unit,
                        "dimensions": metadata["dimensions"],
                        "region": self.region,
                    }
                    records.append(record)

                    # Track max timestamp for cursor (compare as strings in ISO format)
                    if max_timestamp is None or ts_iso > max_timestamp:
                        max_timestamp = ts_iso

            return records, max_timestamp

        def _fetch_metric_data_batch(
            self,
            batch: List[Dict[str, Any]],
            batch_metadata: Dict[str, Dict[str, Any]],
            start_time: datetime,
            end_time: datetime,
        ) -> tuple[List[dict], str | None]:
            """
            Fetch metric data for a single batch, splitting time range into 1-hour chunks.

            Args:
                batch: List of MetricDataQuery dictionaries (up to 500)
                batch_metadata: Dictionary mapping query ID to metric metadata
                start_time: Start time for data retrieval
                end_time: End time for data retrieval

            Returns:
                Tuple of (all_records, max_timestamp):
                - all_records: List of all records from this batch
                - max_timestamp: Maximum timestamp string (ISO 8601) or None
            """
            all_records = []
            max_timestamp = None

            # Split time range into 1-hour chunks to avoid timeouts
            current_start = start_time
            while current_start < end_time:
                current_end = min(current_start + timedelta(hours=1), end_time)

                try:
                    response = self.client.get_metric_data(
                        MetricDataQueries=batch,
                        StartTime=current_start,
                        EndTime=current_end
                    )
                except ClientError as e:
                    error_code = e.response.get("Error", {}).get("Code", "")
                    if error_code == "Throttling":
                        # Retry with exponential backoff (simplified - in production use retry decorator)
                        time.sleep(1)
                        continue
                    raise RuntimeError(f"Failed to get metric data: {e}")

                # Process response
                metric_data_results = response.get("MetricDataResults", [])
                batch_records, batch_max_ts = self._process_metric_data_results(
                    metric_data_results, batch_metadata
                )
                all_records.extend(batch_records)

                # Update max timestamp
                if batch_max_ts:
                    if max_timestamp is None or batch_max_ts > max_timestamp:
                        max_timestamp = batch_max_ts

                # Handle pagination if NextToken is present
                # Note: GetMetricData pagination via NextToken requires making additional requests
                # For simplicity, we process all results in the current response
                # If NextToken is present, it means there are more results but we've hit the limit
                # In production, you might want to handle this by making additional requests

                current_start = current_end

            return all_records, max_timestamp

        def _compute_next_cursor(
            self, max_timestamp: str | None, start_time: datetime
        ) -> str:
            """
            Compute the next cursor from the maximum timestamp.

            Args:
                max_timestamp: Maximum timestamp string (ISO 8601) or None
                start_time: Start time used for this sync (fallback if no max_timestamp)

            Returns:
                Next cursor as ISO 8601 string
            """
            if max_timestamp:
                try:
                    # Parse max timestamp and subtract 5 minutes for lookback
                    if isinstance(max_timestamp, str):
                        # Handle both 'Z' suffix and timezone-aware formats
                        ts_clean = max_timestamp.replace('Z', '+00:00')
                        if '+' not in ts_clean and ts_clean.count('-') > 2:
                            # Assume UTC if no timezone info
                            ts_clean = ts_clean + '+00:00'
                        dt = datetime.fromisoformat(ts_clean)
                        # Convert to UTC naive datetime if timezone-aware
                        if dt.tzinfo is not None:
                            dt = dt.replace(tzinfo=None)
                    else:
                        dt = max_timestamp
                    dt_with_lookback = dt - timedelta(minutes=5)
                    return dt_with_lookback.isoformat()
                except Exception:
                    # Fallback: use max_timestamp as-is
                    return max_timestamp
            else:
                # No records, reuse start_time as cursor
                return start_time.isoformat()

        def read_table(
            self, table_name: str, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read metric data points from CloudWatch.

            This method:
                - Discovers all metrics using ListMetrics API
                - Batches metrics into GetMetricData requests (up to 500 per request)
                - Automatically manages time ranges:
                  - First run: 60 minutes ago to current time
                  - Subsequent runs: from stored cursor (with 5-minute lookback) to current time
                - Returns records with namespace, metric_name, timestamp, value, statistic, unit, dimensions, region

            Optional table_options:
                - period: Period in seconds for metric aggregation (default: 300, must be multiple of 60)
                - statistics: List of statistics to retrieve (default: ["Average"])
                - dimensions: Dimensions filter as dict (optional)
                - unit: Unit for the metric (optional)
            """
            if table_name != "metrics":
                raise ValueError(f"Unsupported table: {table_name!r}")

            # Parse table options
            parsed_options = self._parse_table_options(table_options)
            period = parsed_options["period"]
            statistics = parsed_options["statistics"]
            unit = parsed_options["unit"]

            # Discover all metrics
            discovered_metrics = self._discover_metrics(table_options)

            if not discovered_metrics:
                # No metrics found, return empty iterator with same offset
                if start_offset:
                    return iter([]), start_offset
                return iter([]), {}

            # Determine time range
            start_time, end_time = self._determine_time_range(start_offset)

            # Build MetricDataQueries
            metric_queries, query_metadata = self._build_metric_queries(
                discovered_metrics, period, statistics, unit
            )

            # Split into batches of 500 (GetMetricData limit) and fetch data
            batch_size = 500
            all_records = []
            max_timestamp = None

            for i in range(0, len(metric_queries), batch_size):
                batch = metric_queries[i:i + batch_size]
                batch_metadata = {qid: query_metadata[qid] for qid in [q["Id"] for q in batch]}

                batch_records, batch_max_ts = self._fetch_metric_data_batch(
                    batch, batch_metadata, start_time, end_time
                )
                all_records.extend(batch_records)

                # Update max timestamp across all batches
                if batch_max_ts:
                    if max_timestamp is None or batch_max_ts > max_timestamp:
                        max_timestamp = batch_max_ts

            # Compute next cursor
            next_cursor = self._compute_next_cursor(max_timestamp, start_time)

            # If no new records and we had a start_offset, return same offset
            if not all_records and start_offset:
                next_offset = start_offset
            else:
                next_offset = {"cursor": next_cursor}

            return iter(all_records), next_offset


    ########################################################
    # pipeline/lakeflow_python_source.py
    ########################################################

    METADATA_TABLE = "_lakeflow_metadata"
    TABLE_NAME = "tableName"
    TABLE_NAME_LIST = "tableNameList"
    TABLE_CONFIGS = "tableConfigs"
    IS_DELETE_FLOW = "isDeleteFlow"


    class LakeflowStreamReader(SimpleDataSourceStreamReader):
        """
        Implements a data source stream reader for Lakeflow Connect.
        Currently, only the simpleStreamReader is implemented, which uses a
        more generic protocol suitable for most data sources that support
        incremental loading.
        """

        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.lakeflow_connect = lakeflow_connect
            self.schema = schema

        def initialOffset(self):
            return {}

        def read(self, start: dict) -> (Iterator[tuple], dict):
            is_delete_flow = self.options.get(IS_DELETE_FLOW) == "true"
            # Strip delete flow options before passing to connector
            table_options = {
                k: v for k, v in self.options.items() if k != IS_DELETE_FLOW
            }

            if is_delete_flow:
                records, offset = self.lakeflow_connect.read_table_deletes(
                    self.options[TABLE_NAME], start, table_options
                )
            else:
                records, offset = self.lakeflow_connect.read_table(
                    self.options[TABLE_NAME], start, table_options
                )
            rows = map(lambda x: parse_value(x, self.schema), records)
            return rows, offset

        def readBetweenOffsets(self, start: dict, end: dict) -> Iterator[tuple]:
            # TODO: This does not ensure the records returned are identical across repeated calls.
            # For append-only tables, the data source must guarantee that reading from the same
            # start offset will always yield the same set of records.
            # For tables ingested as incremental CDC, it is only necessary that no new changes
            # are missed in the returned records.
            return self.read(start)[0]


    class LakeflowBatchReader(DataSourceReader):
        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.schema = schema
            self.lakeflow_connect = lakeflow_connect
            self.table_name = options[TABLE_NAME]

        def read(self, partition):
            all_records = []
            if self.table_name == METADATA_TABLE:
                all_records = self._read_table_metadata()
            else:
                all_records, _ = self.lakeflow_connect.read_table(
                    self.table_name, None, self.options
                )

            rows = map(lambda x: parse_value(x, self.schema), all_records)
            return iter(rows)

        def _read_table_metadata(self):
            table_name_list = self.options.get(TABLE_NAME_LIST, "")
            table_names = [o.strip() for o in table_name_list.split(",") if o.strip()]
            all_records = []
            table_configs = json.loads(self.options.get(TABLE_CONFIGS, "{}"))
            for table in table_names:
                metadata = self.lakeflow_connect.read_table_metadata(
                    table, table_configs.get(table, {})
                )
                all_records.append({TABLE_NAME: table, **metadata})
            return all_records


    class LakeflowSource(DataSource):
        def __init__(self, options):
            self.options = options
            self.lakeflow_connect = LakeflowConnect(options)

        @classmethod
        def name(cls):
            return "lakeflow_connect"

        def schema(self):
            table = self.options[TABLE_NAME]
            if table == METADATA_TABLE:
                return StructType(
                    [
                        StructField(TABLE_NAME, StringType(), False),
                        StructField("primary_keys", ArrayType(StringType()), True),
                        StructField("cursor_field", StringType(), True),
                        StructField("ingestion_type", StringType(), True),
                    ]
                )
            else:
                # Assuming the LakeflowConnect interface uses get_table_schema, not get_table_details
                return self.lakeflow_connect.get_table_schema(table, self.options)

        def reader(self, schema: StructType):
            return LakeflowBatchReader(self.options, schema, self.lakeflow_connect)

        def simpleStreamReader(self, schema: StructType):
            return LakeflowStreamReader(self.options, schema, self.lakeflow_connect)


    spark.dataSource.register(LakeflowSource)  # pylint: disable=undefined-variable
